<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v43-2012-12-04.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.3 2012-12-04" file="US20140004498A1-20140102.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20131218" date-publ="20140102"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20140004498</doc-number><kind>A1</kind><date>20140102</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>14017599</doc-number><date>20130904</date></document-id></application-reference><us-application-series-code>14</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>09</class><subclass>B</subclass><main-group>7</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20140102</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>09</class><subclass>B</subclass><main-group>7</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20140102</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><classification-national><country>US</country><main-classification>434350</main-classification></classification-national><invention-title id="d0e43">COMPUTING METHOD AND SYSTEM WITH DETACHED SENSOR IN A WINDOW ENVIRONMENT</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>13831568</doc-number><date>20130314</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>8538321</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>14017599</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>13481821</doc-number><date>20120526</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>8475174</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>13831568</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>10694706</doc-number><date>20031028</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>8398407</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>13481821</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>10050578</doc-number><date>20020114</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>6699043</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>10694706</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>09385795</doc-number><date>19990830</date></document-id><parent-status>ABANDONED</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>10050578</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>08689678</doc-number><date>19960813</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>5944530</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>09385795</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Iplearn-focus, LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Ho</last-name><first-name>Chi Fai</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Tong</last-name><first-name>Peter P.</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>IPLEARN-FOCUS, LLC</orgname><role>02</role><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">One embodiment includes a computer-implemented method using a window environment of a display, with a detached imaging sensor, to enable a user to learn. Another embodiment includes a computer-implemented system helping a user learn using a detached imaging sensor. In yet another embodiment, a computer-implemented system monitors automatically more than once a user's behavior while the user is working on materials. Through monitoring the user's volitional or involuntary behavior, the system determines whether to change what is to be presented by the display. The change could include providing rewards, punishments, and stimulation; or changing the materials. The system can also react by asking the user a question. Based on the user's response, the system may change to more appropriate materials, or different presentation styles.</p></abstract></us-patent-application>