<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v43-2012-12-04.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.3 2012-12-04" file="US20140003663A1-20140102.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20131218" date-publ="20140102"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20140003663</doc-number><kind>A1</kind><date>20140102</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>13997310</doc-number><date>20110411</date></document-id></application-reference><us-application-series-code>13</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20140102</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00228</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20140102</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><classification-national><country>US</country><main-classification>382103</main-classification></classification-national><invention-title id="d0e43">METHOD OF DETECTING FACIAL ATTRIBUTES</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Jianguo</first-name><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Tao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="02" app-type="applicant" designation="us-only"><addressbook><last-name>Du</last-name><first-name>Yangzhou</first-name><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="03" app-type="applicant" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Qiang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Jianguo</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Tao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Du</last-name><first-name>Yangzhou</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Qiang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN11/72597</doc-number><kind>00</kind><date>20110411</date></document-id><us-371c124-date><date>20130916</date></us-371c124-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Detection of a facial attribute such as a smile or gender in a human face in an image is performed by embodiments of the present invention in a computationally efficient manner. First, a face in the image is detected to produce a facial image. Facial landmarks are detected in the facial image. The facial image is aligned and normalized based on the detected facial landmarks to produce a normalized facial image. Local features from selected local regions are extracted from the normalized facial image. A facial attribute is predicted in each selected local region by inputting each selected local feature into a weak classifier having a multi-layer perceptron (MLP) structure. Finally, output data is aggregated from each weak classifier component to generate all indication that the facial attribute is detected in the facial image.</p></abstract></us-patent-application>