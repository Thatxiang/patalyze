<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v43-2012-12-04.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.3 2012-12-04" file="US20140003526A1-20140102.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20131218" date-publ="20140102"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20140003526</doc-number><kind>A1</kind><date>20140102</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>14003924</doc-number><date>20120222</date></document-id></application-reference><us-application-series-code>14</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>11157462.0</doc-number><date>20110309</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>36</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20140102</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>19</main-group><subgroup>00733</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20140102</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><classification-national><country>US</country><main-classification>37524016</main-classification></classification-national><invention-title id="d0e61">METHOD FOR CODING A SEQUENCE OF DIGITIZED IMAGES</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><last-name>Amon</last-name><first-name>Peter</first-name><address><city>Munchen</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Amon</last-name><first-name>Peter</first-name><address><city>Munchen</city><country>DE</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP12/52974</doc-number><kind>00</kind><date>20120222</date></document-id><us-371c124-date><date>20130909</date></us-371c124-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">In a method for coding a sequence of digitized images, a motion compensation process is performed, the motion compensation process using motion vectors between image blocks referring to a number of pixels in a current image and reference blocks referring to a number of pixels in a reference image. The reference image is based on one or more images out of the sequence. For each image block of at least a part of the current image a temporal prediction based on a corresponding reference block indicated by a motion vector is performed, resulting in a prediction error between the image block and the corresponding reference block, where the prediction error is coded. In the method of the invention, the current image in the sequence is divided into several image areas, with a reference area in the reference image being associated with each image area. The temporal prediction of an image block in an image area is based on a reference block at least partially located in the reference area associated with the image area and including pixel information from this reference area.</p></abstract></us-patent-application>